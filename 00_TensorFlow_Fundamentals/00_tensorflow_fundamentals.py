# -*- coding: utf-8 -*-
"""00_TensorFlow_Fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsenmMBT0vvV_LRwT1pelpElTdTbU-I1

# TensorFlow Fundamentals

-----

## What Is Deep Learning?

* A type of machine learning based on artificial neural networks in which multiple layers or processing are used to extract progressively higher level features from data. 
* Machine learning is turning things (data) into numbers and finding patterns in those numbers. The computer does this part through code and math.
* Deep learning is usually considered a subfield of machine learning. Both machine learning and deep learning fall under the umbrella of artificial intelligence.

The goal of this course is to get deep into the programming element of deep learning using the `TensorFlow` library.

## Why Use Machine Learning/Deep Learning?

* The good reason is: why not? Both are extremely powerful tools.
* The better reason: For a complex problem, such as teaching a car how to self-drive, can you think of all the rules? Probably not, since there are so many of them and many of the rules are learned through driving itself.
* Machine learning and deep learning are very flexible and can be used to solve a huge different variety of problems.
* You should not use ML/DL if you can build a simple rule-based system that doesn't require machine learning, do that. This comes from Google's ML Handbook.

### What Is DL Good For?

* Problems with long lists of rules - when the traditional approach fails, use ML/DL.
* Continually changing environments - deep learning can adapt (learn) to new scenarios.
* Discovering insights within large collections of data - can you imagine trying to hand-craft rules for what 101 different kinds of food look like?

### What Is DL (Typically) Not Good For?

* When you need explainability - the patterns learned by a deep learning models are typically uninterpretable by a human.
* When the traditional approach is a better problem.
* When errors are unnacceptable - since the outputs of deep learning model aren't always predictable. 
* When you don't have much data - DL models usually require a fairly large amount of data to produce great results.

### ML vs DL

Traditional machine learning algorithms typically perform well on structured data, with rows and columns. Think of a typically dataset on an excel sheet. Deep learning performs well on unstructured data, such as natural language text or sound waves. 

Some of the most common algorithms for ML are: random forest, naive bayes, nearest neighbor, and SVM. These can be referred to as "shallow algorithms."

Some of the most common algorithms for DL are: neural networks, fully connected neural network, CNN, RNN, transformer, and others. For this course, we are going to be focused on building NNs, fully connected NNs, CNNs, and RNNs. 

You can use ML and DL for structured/unstructured data; it just depends on how you formulate the problem.

## What Are Neural Networks?

* They are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.
* These NNs are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. 
* If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed to the next layer of the network. 
* Input of the NN must be numeric and we typically set the output as something that a human can understand. For example, we can have a probability output with anything above 0.5 as an event happening and less than or equal to 0.5 as an event not happening. 

Essentially, the neural networks takes inputs, converts them to numerical values (numerical encoding), learngs representation (patterns, features, and weights), creates representation outputs (numeric values, such as probabilities), and then converts them to human understandable outputs (such as an event happening or not). 

As you can see, there is an input later (data goes in there), hidden layers (learns patterns in the data), and an output layer (outputs learned representation or predicted probabilities). 

As a note, patterns in data is an arbitrary term, you will also hear embedding, weights, feature representation, and feature vectors. This all refer to similar things.

Types of learning:

* Supervised Learning - we have data and the output labels.
* Semi-supervised Learning - we have data and it only has some labels.
* Unsupervised Learning - we have data and no output labels.
* Transfer learning - taking what one DL model has learned on a set of data and using it on another problem from another set of data. 

## What Is Deep Learning Already Used For?

* Recommendation (Such as YouTube)
* Translation
* Speech Recognition
* Computer Vision
* Natural Language Processing 

## What Is and Why Use TensorFlow? 

`TensorFlow` will help us learn deep learning models. `TensorFlow` is:

* End-to-end platform for machine learning
* Write fast deep learning code in Python/other accessible languages (able to run on a GPU/TPU)
* Able to access many pre-built deep learning models (TensorFlow Hub)
* Whole stack: preprocess data, model data, deploy model in your application
* Originally designed and used in-house by Google (now open-source)

So, why `TensorFlow`?

* Easy model building
* Robust ML production anywhere
* Powerful experimentation for research 

`Keras` is part of `TensorFlow`. It can be used by anyone to solve a problem in a day without an investing, without an engineering team, and without expensive hardware. 

### What Is a GPU/TPU?

* A GPU is a graphics processing unit. It is the graphics card in your computer. These are very fast at crunching numbers; they do numerical calculations very fast. 
* A TPU is a tensor processing unit. It is an AI accelerator developed by Google for neural network machine learning, using Google's own `TensorFlow` software. 

## What Is a Tensor?

A tensor is a numerical way to represent information. They are multi-dimensional arrays with a uniform type (called a `dtype`). If you are familiar with `NumPy`, tensors are kind of like `np.arrays`. 

All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one. 

## What Else Are We Going To Cover?

* `TensorFlow` basics and fundamentals.
* Preprocessing data (getting it into tensors)
* Building and using pretrained deep learning models
* Fitting a model to the data (learning patterns)
* Making predictions with a model (using patterns)
* Evaluating model predictions
* Saving and loading models
* Using a trained model to make predictions on custom data

The general workflow when working with `TensorFlow` is:

1. Get data ready (into tensors)
2. Build or pick a pretrained model (to suit your problem)
3. Fit the model to the data and make a prediction
4. Evaluate the model
5. Improve through experimentation
6. Save and reload your trained model

##  How To Approach This Course:

1. Write code.
2. Explore & experiment.
3. Ask questions.
4. Do the exercises (try them yourself before looking at the solutions).
5. If you want to learn more, look stuff up. 
6. Share your work.
7. Avoid overthinking and "I can't learn it" mentality. 

-----

## Creating Your First Tensors

In this section, we will cover some of the fundamental concepts of tensors with `TensorFlow`. 

More specifically, we are going to cover:

* Introduction to tensors
* Getting information from tensors
* Manipulating tensors
* Tensors and `numpy`
* Using `@tf.function`, which is a way to speed up your regular Python functions in `TensorFlow`
* Using GPUs with TensorFlow (or TPUs)
* Exercises to try for yourself
"""

# Import tensorflow library
import tensorflow as tf

# Check version
print(tf.__version__)

"""### Creating Tensors With `tf.constant()`

Tensors are multi-dimensional arrays with a uniform type (called a dtype).

If youâ€™re familiar with R array or NumPy, tensors are (kind of) like R or NumPy arrays.

All tensors are immutable: you can never update the contents of a tensor, only create a new one.

We will not create our first tensors with `tf.constant()`. There are different ways of creating tensors, but most tensors you work with you will not create. This is because `TensorFlow` has many built-in modules that can read in data sources and convert them to tensors, if you write the code. 

However, we first want to get familiar with tensors.

`tf.constant()` creates a constant tensor from a tensor-like object. 
"""

# Creating our first tensor with tf.constant()
scalar = tf.constant(7)
scalar

# Check the number of dimensions of a tensor (ndim = # of dimensions) 
# .ndim displays the numbers of axes in a tensor 
# For example, a vector has just a row, so 1 axis, a matrix has (rows x columns) so 2 axes
scalar.ndim

# Create a vector 
vector = tf.constant([10, 10])
vector

# Check the dimensions of the vector
vector.ndim

# Create a matrix (has more than one dimension)
matrix = tf.constant([[10,7],
                      [7,10]])
matrix

# Check the dimensions of the matrix
matrix.ndim

# Creating a matrix and specifying the data type with dtype parameter
another_matrix = tf.constant([[10.,7.], 
                              [3.,2.],
                              [8.,9.]], dtype = tf.float16)
another_matrix

"""The reason that we are learning the `dtype =` parameter is because, when we use `tf.constant()`, the default data type with bill `int32`. 

If you use smaller numbers at the end of the data types, such as `float16`, the precision (how the number is stored on your computer) will be less, so it will take up less memory. 
"""

# Checking the number of dimensions of another_matrix
another_matrix.ndim

"""Even though the shape is (3,2) for `another_matrix`, the number of dimensions is 2. The total number of dimensions is the number of elements that is in the shape. 

So, a scalar will have no dimensions, a vector 1 dimension, and a matrix 2 dimensions.

How can we increase the number of dimensions?
"""

# Let's create a tensor
tensor = tf.constant([[[1,2,3],
                       [4,5,6]],
                       [[7,8,9],
                       [10,11,12]],
                       [[13,14,15],
                       [16,17,18]]])
tensor

"""Now we have an extra element in the shape. It is now (3,2,3)."""

# Check the number of dimensions for the tensor
tensor.ndim

"""The important thing to note is that, even though we have given them different names above, we will always refer to them as tensors. 

What we have created so far:

* Scalar = single number
* Vector = number with direction/list of numbers
* Matrix = 2-dimensional array of numbers
* Tensor = an n-dimensional array of numbers (where n can be any number, a 0-dimensional tensor is a scalar, a 1-dimensional tensor is a vector)

### Creating Tensors With `tf.Variable`
"""

# Create the same tensor with tf.Variable as above
changeable_tensor = tf.Variable([10,7])
unchangeable_tensor = tf.constant([10,7])
changeable_tensor, unchangeable_tensor

# What happens if we try to change an element in our changeable tensor?
changeable_tensor[0] = 7
changeable_tensor

# How about we try .assign()
changeable_tensor[0].assign(7)
changeable_tensor

# Let's try to change our unchangeable tensor
unchangeable_tensor[0] = 7

# Now let's try assign
unchangeable_tensor[0].assign(7)
unchangeable_tensor

"""So, why can we change some tensors and why can we not change others? When we are writing neural network code, we may want some of our tensor values to be changed and some not changed. So, there is a variable tensor, which can be changed, and a constant tensor, which cannot be changed.

A lot of times you won't have to make the decision to go between a variable tensor and a constant tensor. The decision will be made for you when `TensorFlow` creates tensors for your neural network.

If ever in doubt, use `tf.constant` and change it later if needed. 

### Creating random tensors

Random tensors are tensors of some arbitrary size which contain random numbers. 

Why would we ever want to create a tensor filled with random numbers? We can use random tensors to randomlyinitialize the weights of our NN. 
"""

# Creating two random (but the same) tensors
random_1 = tf.random.Generator.from_seed(42) # set seed for reproducibility
random_1 = random_1.normal(shape=(3, 2)) 
random_1

"""We have used `.normal` here, but we can also use `.uniform`. The first will give us values from the normal distribution, while the second will give us values from a uniform distribution. The uniform distribution has a constant probability. 

In practice, when writing random tensors, a lot of this will be done behind the scenes.
"""

# Creating another random tensor drawing from uniform distribution
random_2 = tf.random.Generator.from_seed(42)
random_2 = random_2.uniform(shape = (3,2))
random_2

# Are the tensors equal?
random_1 == random_2

"""So, when drawing from different distributions, the tensors will be different. One thing to note that, is when you set the seed, then the tensors are pseudo-random since it will generate the random numbers from that initialization point. If we change the seed, it will change the numbers.

### Shuffling the Order of Elements in a Tensor

When a NN starts to learn, the weights are typically initialized with random numbers. Why would we want to shuffle the order of elements in a tensor? We want 
want to shuffle the order of our input tensor (data) so that the order of the input data does not affect the way the NN learns. For example, if we have 10,000 of one image first, then 5,000 of another, we do not want our NN to learn its parameters solely based off the the first 10,000 at once then be exposed to the last 5,000. Thus, we can shuffle the order so it can learn from both images at the same time. 
"""

# Shuffle a tensor (valuable for when you want to shuffle your data so the data so the inherent order does not affect learning)
# Making the tensor
not_shuffled = tf.constant([[10,7],
                            [3,4],
                            [2,5]])
not_shuffled.ndim

not_shuffled

# Shuffling the tensor - shuffles a tensor along the first dimension, in this case rows
# The elements in the second dimension, columns, stay in the same position 
tf.random.shuffle(not_shuffled)

# Shuffling with a seed set so we can reproduce the results
tf.random.shuffle(not_shuffled, seed = 42)

"""However, when shuffling this setting `seed` in `.shuffle` does not always give the same order. Why is this happening? There are 2 seeds in `TensorFlow`, global and operational."""

# Global level seed
tf.random.set_seed(42)

# Operation level seed
tf.random.shuffle(not_shuffled, seed = 42)

"""Homework: Read through the `TensorFlow` documentation about random seed generation: https://www.tensorflow.org/api_docs/python/tf/random/set_seed and practice writing 5 random tensors and shuffling them. In other words, create some tensors using `tf.constant` and try to get the shuffle order using `tf.random.set_seed` and a combination of `seed =`. 

Instead of doing this, we will look at an example from stack overflow: https://stackoverflow.com/questions/61078946/how-to-get-reproducible-results-keras-tensorflow

Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. `tf.random.set_seed()` sets the global seed.

Its interactions with operation-level seeds are as follows: 

1. If neither the global seed nor the operation seed is set: A randomly picked seed is used for this operation.
2. If the operation seed is not set but the global seed is set: The system picks an operation seed from a stream of seeds determined by the global seed.
3. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence.
4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.

First Scenario:

A random seed will be picked by default. This can be easily noticed with the results. It will have different values every time you re-run the program or call the code multiple times.
"""

x_train = tf.random.normal((10,1), 1, 1, dtype=tf.float32)
print(x_train)

"""Second Scenario:

The global is set but the operation has not been set. Although it generated a different seed from first and second random. If you re-run or restart the code. The seed for both will still be the same. It both generated the same result over and over again.
"""

tf.random.set_seed(2)
first = tf.random.normal((10,1), 1, 1, dtype=tf.float32)
print(first)
sec = tf.random.normal((10,1), 1, 1, dtype=tf.float32)
print(sec)

"""Third Scenario:

For this scenario, where the operation seed is set but not the global. If you re-run the code it will give you different results but if you restart the runtime if will give you the same sequence of results from the previous run.
"""

x_train = tf.random.normal((10,1), 1, 1, dtype=tf.float32, seed=2)
print(x_train)

"""Fourth Scenario:

Both seeds will be used to determine the random sequence. Changing the global and operation seed will give different results but restarting the runtime with the same seed will still give the same results.
"""

tf.random.set_seed(3)
x_train = tf.random.normal((10,1), 1, 1, dtype=tf.float32, seed=1)
print(x_train)

"""By setting the global seed, it always gives the same results.

From the course, it looks like, if we want our shuffled tensors to be in the same order, we have to use the global level random seed as well as the operation level random seed.

We need these seeds so our NN experiments yield the same results. Since the parameter intialization is random, it can yield different results without the same seed. 

### Creating Tensors From `NumPy` Arrays

For many `numpy` operations, `TensorFlow` has similar operations. 
"""

# Making a tensor full of ones
tf.ones([10,7])

# Create a tensor of all zeros - the shape can also be in parantheses
tf.zeros((10,7))

"""The main difference between `numpy` arrays and `TensorFlow` tensors is that tensors can be run on a GPU (much faster for numerical computing)."""

# You can also turn numpy arrays into tensorflow tensors
# X = tf.constant(matrix); usually capital
# y = tf.constant(vector); usually lowercase
import numpy as np
numpy_A = np.arange(1, 25, dtype = np.int32) # Create a numpy array between 1 and 25

# How do you convert this to a tensor?
A = tf.constant(numpy_A) # We can just put the numpy array in the tf.constant() function
A

# What if we wanted to change the shape? For example, making the array a 3-D tensor
# 
A = tf.constant(numpy_A, shape = (2, 3, 4))
B = tf.constant(numpy_A)
A, B

"""If you want to readjust the shape of the tensor or `numpy` array, the new shape in `shape = ()` must add up to give the same number of elements in the original tensor. So, above, we put `(2,3,4)` and we were able to reshape the array since 2\*3\*4 = 24, which has the same amount of elements as the original array. However, if we change the 4 to a 5, we will get an error: """

# Changing 4 to 5 to demonstrate
A = tf.constant(numpy_A, shape = (2, 3, 5))

"""So, the above error says the new shape would have 30 elements (2\*3\*5) rather than 24, which is the size of the original array. We can also set the shape to (3,8), since 3\*8 = 24."""

A = tf.constant(numpy_A, shape = (3, 8))
A

"""### Getting Information From Tensor (Tensor Attributes)

Tensors can run on a GPU, whereas `numpy` arrays cannot. There will be times where you want to get attributes from your tensors.

When dealing with tensors, you want to be aware of the following attributes:

* Shape: the # elements of each dimension of a tensor.
* Rank: the number of dimensions; scalar has 0, vector has 1, matrix has 2, tensor has n.
* Axis or Dimension: a particular dimension of a tensor. How you access a particular axis of a tensor.
* Size: The total number of items in the tensor. 
"""

# Create a rank 4 tensor - has 4 dimensions
rank_4_tensor = tf.zeros([2, 3, 4, 5])
rank_4_tensor

"""Typically, when building NNs, you will spend a lot of time adjusting the shape of your tensors. This is because they have to be a in a specific shape when you pass it into the NN. """

# 0 element of rank 4 tensor
rank_4_tensor[0]

# Rank 4 tensor shape, dimensions, size
rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)

"""The size of the vector will be the product of all the shape inputs. So, in this case, it is 120 since 2\*3\*4\*5."""

# Get various attributes of our tensor with pretty print
print("Datatype of very element:", rank_4_tensor.dtype)
print("Number of dimensions (rank):", rank_4_tensor.ndim)
print("Shape of tensor:", rank_4_tensor.shape)
print("Elements along the 0 axis:", rank_4_tensor.shape[0])
print("Elements along the last axis:", rank_4_tensor.shape[-1])
print("Total number of elements in our tensor:", tf.size(rank_4_tensor))

"""To convert the output of the size attribute for our tensor we can convert it to a `numpy` integer: """

print("Total number of elements in our tensor:", tf.size(rank_4_tensor).numpy())

"""We learned how to get the attributes from the tensors we create or are dealing with. Often times with NNs, we will be dealing with tensors that we cannot visualize since they are very large. 

### Indexing And Expanding Tensors

Tensors can be indexed just like Python lists.
"""

# Get the first 2 elements of each dimension in our tensor
rank_4_tensor = tf.zeros([2, 3, 4, 5])
rank_4_tensor[:2, :2, :2, :2]

# Get the first element from each dimension from each index but the final one
rank_4_tensor[:1, :1, :1, :] # the single colon will mean get the whole thing

# Another example
rank_4_tensor[:1, :1, :, :1]

# Adding an extra dimensions to the end of a tensor 
# Create a rank 2 tensor (2 dimensions)
rank_2_tensor = tf.constant([[10, 7],
                             [3,4]])

rank_2_tensor.shape, rank_2_tensor.ndim

# Get the last item of each row for our rank 2 tensor
rank_2_tensor[:, -1]

# Adding an extra dimension onto our rank 2 vector
# This will be more thoroughly covered when we do NNs, since the tensors will need a specific shape
rank_3_tensor = rank_2_tensor[..., tf.newaxis]
rank_3_tensor

"""The `...` means every previous axis. It would be the same as: `rank_2_tensor[:, :, tf.newaxis]`"""

# Alternative to tf.newaxis
tf.expand_dims(rank_2_tensor, axis = -1) # -1 means expand on the final axis

"""The input for `tf.expand_dims()` is a tensor. The `axis = ` is an integer specifying the dimension index at which to expand the shape of input. Giving an input of D dimensions."""

# What is the axis is 0?
tf.expand_dims(rank_2_tensor, axis = 0) # expand the 0 axis

# Expand the dimension in the middle
tf.expand_dims(rank_2_tensor, axis = 1)

"""The numbers within the tensor stay the same when you add a new axis, but how the numbers are stored changes. 

### Manipulating Tensors With Basic Operations

When building models in `TensorFlow`, much of the pattern discovery within tensors is done for you. Much of that pattern discovery is with the extended us of a few basic operations.

**Basic Operations:**

`+`, `-`, `*`, `/`
"""

# You can add values to a tensor using the addition operator
tensor = tf.constant([[10, 7],
                      [3, 4]])
tensor + 10

# Original tensor is unchanged
# This is important because when we are manipulating tensors, we do not always want to change 
# The underlying tensor
tensor

# To change the underlying tensor you can:
tensor = tensor + 10
tensor

# Multiplication also works
tensor * 10

# Subtraction
tensor - 10

# Division
tensor/10

"""In addition to the Python operators, we can use built-in `TensorFlow` functions."""

# We can use the tensorflow built-in function too
print(tf.multiply(tensor, 10))
print(tf.divide(tensor, 10))
print(tf.add(tensor, 10))
print(tf.subtract(tensor, 10))
print(tf.square(tensor, 2))

# For certain operations, like exponentiation or square root, the data must be a float, not an integer
tensor_float = tf.cast(tensor, dtype=  tf.float16)
print(tf.exp(tensor_float))
print(tf.sqrt(tensor_float))

"""If you have to do some tensor operation, it is quicker to use the `TensorFlow` functions since it can be accelerated via the GPU. 

### Matrix Multiplication With Tensors

In machine learning, matrix multiplication is one of the most common tensor operations. The basic operations are referred to as element-wise operations. This means that the operation goes through one element at a time. So, if we are adding 10 to a tensor that is a 2x2 matrix, it will add 10 to each individual element. 

Matrix multiplication is not element wise. When you multiply a matrix (tensor) by another matrix (tensor), you need to calculate the "dot product" of the rows and columns. 

The dot product is where we multiply and then sum matching numbers. In order for 2 matrices to be multiplied, the number of rows in the second matix must be equal to the number of columns in the first matrix. The size of the new matrix will be the number of rows from the first matrix and the number of columns in the second matrix. 
"""

# Matrix multiplication in tensorflow
tensor = tf.constant([[10, 7],
                      [3, 4]])
tf.matmul(tensor, tensor)

# Different output using the * operator; this is because it will multiply them element-wise
# It will not give us dot products 
tensor * tensor

# Matrix multiplication with Python operator "@"
# The "@" is for matrix multiplication
tensor @ tensor

# Practice problem with matrices of different sizes
# The rows in tensor2 = columns in tensor1
# The new matrix will be 3x2
tensor1 = tf.constant([[1,2,5],
                       [7,2,1],
                       [3,3,3]])
tensor2 = tf.constant([[3,5],
                       [6,7],
                       [1,8]])
tf.matmul(tensor1, tensor2)

# Trying matrix multiplication with incompatible sizes
# This will give us an error since tensor1 has 2 columns and tensor2 has 3 rows
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[3,5],
                       [6,7],
                       [1,8]])
tf.matmul(tensor1, tensor2)

"""There are 2 rules our tensors (matrices) need to fulfill if we are going to multiply them:

1. The inner dimensions must match
2. The resulting matrix has the shape of the outer dimensions 
"""

# Fixing the previous matrices so they can be multiplied
# Will delete one row from tensor2 so 3x2 * 2x2
# The resulting matrix will be 3x2
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[3,5],
                      [6,7]])
tf.matmul(tensor1, tensor2)

# We can also do
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[3,5],
                       [6,7],
                       [1,8]])
# Reshape tensor2
tensor2 = tf.reshape(tensor2, shape=(2,3))

# Check the shape of our tensors
tensor1.shape, tensor2.shape

# Now performing matrix multiplication
tf.matmul(tensor1, tensor2)

# We can also do 
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[3,5],
                       [6,7],
                       [1,8]])
# Rehshape tensor1
tensor1 = tf.reshape(tensor1, shape=(2,3))

# Check the shape of our tensors
tensor1.shape, tensor2.shape

# Now performing matrix multiplication
tensor1 @ tensor2

"""The above exercise has shown that the result of the matrix multiplication depends on which tensor you ultimately reshape, if the dimensions for matrix multiplication are incompatible. """

# What if we use transpose rather than reshape?
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])

tensor1, tf.transpose(tensor1), tf.reshape(tensor1, shape=(2,3))

"""As you can see, transposing and reshaping gives a matrix with the same dimensions, but the numbers are in a different order. The difference between tranpose and reshape is that transpose flips the axes, whereas reshape shuffles the tensor around into the shape we want. 

As a note: The transpose $A^{T}$ of a matrix $A$ can be obtained by reflecting the elements along its main diagonal. Repeating the process on the transposed matrix returns the elements to their original position.
"""

# Try matrix multiplication with transpose rather than reshape
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[3,5],
                       [6,7],
                       [1,8]])

tf.matmul(tf.transpose(tensor1), tensor2)

"""This is just an example of the type of things and manipulation you will be doing with tensors when inputting them into your NN and after you get the output. 

Matrix multiplication is also referred to as the dot product. 

You can perform matrix multiplication:

* `tf.matmul()`
* `tf.tensordot()`
* `@`
"""

# Perform the dot product on X and Y (requires X or Y to be tranposed)
tensor1 = tf.constant([[1,2],
                       [3,4],
                       [5,6]])
tensor2 = tf.constant([[7,8],
                       [9,10],
                       [11,12]])
tf.tensordot(tf.transpose(tensor1), tensor2, axes=1)

"""`tf.tensordot()` is tensor contraction of a and b along specified axes and out product. It sums the product of elements from a and b over the indices specified by axes."""

# Matrix multiplication by between tensor1 and tensor2 (transpose tensor2)
tf.matmul(tensor1, tf.transpose(tensor2))

# Perform matrix multiplication between tensor1 and tensor2 (tensor2 reshaped)
tf.matmul(tensor1, tf.reshape(tensor2, shape=(2,3)))

"""As you can see, we get different results for transpose and reshape. We will spent a lot of time reshaping and transposing tensors when building our NN, so that is why we are practicing it now."""

# Check values of tensor2, reshape tensor2, and transposed tensor2
print("Normal Y:")
print(tensor2, "\n") # "\n" is for new line

print("Y reshaped to (2,3):")
print(tf.reshape(tensor2, shape=(2,3)))

print("Y transposed:")
print(tf.transpose(tensor2))

"""Writing print statements like above is helpful to investigate when you get errors. Additionally, if an operation works but the results are incorrect, this is called a silent error. Writing print statements like this is a great way to investigate silent errors. """

# Matrix multiplication
tf.matmul(tensor1, tf.transpose(tensor2))

"""So, which one should you use: transpose or reshape? Well, `TensorFlow` will do a lot of the work for you in the background. Generally, with matrix multiplication on 2 tensors where the shapes don't line up, you will transpose one of the tensors rather than reshaping them to satisfy the matrix multiplication rules. 

### Changing the Datatype of Tensors

The default data type of most tensors with be `int32`, depending on how they have been created. Sometimes, you will want to change the data type.
"""

# Create a new tensor with default datatype (float32)
# The default datatype depends on the contents on the tensor
B = tf.constant([1.7, 7.4])
B.dtype

# Create another tensor with default datatype (int32)
C = tf.constant([1,2])
C.dtype

# Change from float 32 to float16 (reduced precision)
# Mixed precision is the use of both 16-bit and 32-bit floating type
# In models to make them run faster and use less memory 
D = tf.cast(B, dtype = tf.float16)
D

"""`TensorFlow` mixed precision: https://www.tensorflow.org/guide/mixed_precision

In `TensorFlow`, 32 bit precision is the default. Modern accerlators can run operations quicker on 16 bit precision. 
"""

# From int32 to float32
E = tf.cast(C, dtype = tf.float32)
E

# From float32 to float16
F = tf.cast(E, dtype = tf.float16)
F

"""### Aggregating Tensors

Aggregating tensors = condensing them from multiple values down to a smaller amount of values. 
"""

# Create new tensor
D = tf.constant([-7, -10])
D

# Get the absolute values
# Take all the negative values and make them positive 
tf.abs(D)

"""Let's go through the following forms of aggregation:

* min
* max
* mean
* sum
"""

# Create a random tensor with values between 0 and 100 of size 50
E = tf.constant(np.random.randint(0, 100, size = 50))
E

# Checking the size, shape, and dimensions of tensor
tf.size(E), E.shape, E.ndim

# Find the minimum
tf.reduce_min(E)

"""The `reduce_` is usually in front of aggregation methods in `TensorFlow`."""

# Find the maximum
tf.reduce_max(E)

# Find the mean
tf.reduce_mean(E)

# Find the sum
tf.reduce_sum(E)

# Find the standard deviation
E_float = tf.cast(E, dtype = tf.float16)
tf.math.reduce_std(E_float)

# Finding the variance of our E tensor
tf.math.reduce_variance(E_float)

# Also can do this
import tensorflow_probability as tfp
tfp.stats.variance(E)

"""### Tensor Troubleshooting Example (Updating Tensor Datatypes) 

This is done above. Some certain function in `TensorFlow` require certain datatypes. For example, they need to be float rather than integer. This was also done earlier with specific math operations, such as square root and exponentiation. 

If you have an error regarding datatype, just look at the documentation and see what they are using in their examples. 

Typically, with `TensorFlow`, `float32` is the most commonly use data type. 

### Finding the Positional Minimum and Maximum of a  Tensor (`argmin` and `argmax`)

When might this be helpful? This will be useful when the neural network provides us with output probabilities. Often times, the outputs are referred to as prediction probabilities. Finding the positional minimum and maximum will be helpful when determining the highest or lowest class probabilities. 
"""

# Create a new tensor for finding positional minimum and maximum
tf.random.set_seed(42)
F = tf.random.uniform(shape=[50])
F

# Finding the maximum element position of F
# At which index does the maximum value occur?
# This is the same as np.argmax()
tf.argmax(F)

# Indexing the tensor on our largest value position
F[tf.argmax(F)]

# Check for equality 
F[tf.argmax(F)] == tf.reduce_max(F)

# Finding the minimum element position of F
# At which index does the minimum value occur?
# This is the same as np.argmin()
tf.argmin(F)

# Indexing the tenosor on our smallest value position
F[tf.argmin(F)]

# Checking for equality
F[tf.argmin(F)] == tf.reduce_min(F)

"""### Squeezing a Tensor (Remove All Single Dimensions)


"""

# Create a tensor to get started
tf.random.set_seed(42)
G = tf.constant(tf.random.uniform(shape=[50]), shape=(1,1,1,1, 50))
G

G.shape

# Squeezing the tensor
G_squeezed = tf.squeeze(G)
G_squeezed, G_squeezed.shape

"""If your tensor has too many single dimensions and you want to reduce it, getting rid of the extra dimensions, you can use the squeeze method.

# One-Hot Encoding a Tensor

One-hot encoding is the same as creating dummy variables. This is used for converting categorical variables into numeric values. One-hot encoding is where the integer encoded variable is removed and a new binary variable is added for each unique integer value. We do this because the input needs to be numeric; the algorithm will not understand words.
"""

# Create a list of indices
some_list = [0,1,2,3] # The locations are represented by indices

# One hot encode our list of indices - this needs the indices and depth parameter
tf.one_hot(some_list, depth = 4)

# Specify custom values for one hot encoding
tf.one_hot(some_list, depth = 4, on_value="Yes", off_value = "No")

"""`tf.one_hot` accepts a list of category indices and depth which is essentially the number of unique categories and also the number of columns in the resulting tensors.

`tf.one_hot()` takes an indices and a depth. The `indices` are the values to actually convert to a one-hot encoding. `Depth` refers to the maximum value to utilize.

The `depth` parameter should be equivalent to the maximum value present in the array, +1 for 0. In this case, the maximum value is 3, so there are four possible values in the encoding - 0, 1, 2, and 3. As such, a `depth` of 4 is required to represent all of these values in the one-hot encoding.
"""

indices = [0,1,1,2,3,0,3]
cat_count = 4  #depth or the number of categories
input = tf.one_hot(indices, cat_count) #apply one-hot encoding 
print(input.numpy())

"""### Trying More Math Operations

There are a huge list of math operations. Let's practice some more common ones:

* Square
* Log 
* Square Root
"""

# Creating a new tensor
H = tf.range(1,10)
H

# Squaring 
tf.square(H)

# Square root - method requires non integer type
H_float = tf.cast(H, dtype = tf.float16)
tf.sqrt(H_float)

# Log - method requires non integer type
tf.math.log(H_float)

"""### Exploring `TensorFlow` and `Numpy` Compatibility 

`TensorFlow` interacts beautifully with `numpy` arrays. The are fully operational with one another. 
"""

# Create a tensor directly from a numpy array
J = tf.constant(np.array([3., 7., 10.]))
J

# Convert our tensor to a np array
np.array(J), type(np.array(J))

# Convert tensor J to a numpy array 
J.numpy(), type(J.numpy)

"""This might be helpful because if there is some functionality that we want to use on our tensors, we can just convert it to a `numpy` array then back."""

# J as a single number
J = tf.constant([3.])
J.numpy()[0]

# The default types of each are slightly different
numpy_J = tf.constant(np.array([3., 7., 10.]))
tensor_J = tf.constant([3., 7., 10.])

# Check the data types of each
numpy_J.dtype, tensor_J.dtype

"""The default type for `numpy` is `float64`. The default type for `TensorFlow` is `float32`. This is important because we may need certain data types of certain operations. So, just be aware that the data types are different. The `float32` type is used for faster numerical processing.

### Making Sure Tensors Work Run Fast on GPUs

A `TensorFlow` tensor can be run on a GPU or TPU, while a `numpy` array cannot. GPUs and TPUs can perform calculations extremely quickly. 

"""

# Check tensorflow gpu usage
tf.test.is_gpu_available # This is deprecated

# Another way
tf.config.list_physical_devices() # We are currently using the CPU

# Another way
tf.config.list_physical_devices('GPU') # There will be none

"""In Google Colab, by default, you will not have access to a GPU/TPU. If you want to use a GPU/TPU, change the runtime type to a GPU/TPU. Using a TPU is a bit more complicated than a GPU as it requires more setup. """

# Check GPU
import tensorflow as tf
tf.config.list_physical_devices('GPU')

# Check the type of GPU you using
!nvidia-smi

"""How do I get my `TensorFlow` code to run on the GPU? If you have access to a CUDA-enabled GPU (Google Colab does), `TensorFlow` automatically uses it whenever possible. 


"""