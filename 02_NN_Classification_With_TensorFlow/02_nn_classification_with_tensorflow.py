# -*- coding: utf-8 -*-
"""02_NN_Classification_With_TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAUNS9NN5pa2RrglC39oMnsHVtTUV9Mz

# Neural Network Classification With TensorFlow

In this tutorial, we will cover how to use NNs to solve classification problems. Classification problems require us to put an observation into a category.

Examples:

* Is an email spam or not spam?
* Is the photo presented sushi, steak, or pizza?
* Will a lender default on a loan?
* Is the species of the flower iris, setosa, or virginica?

From the examples above, you can see that classification problems can be binary (only 2 possible outcomes/groups) or multiclass/multinomial (more than 2 possible outcomes/groups). Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.

There is also multilabel classification. Multilabel classification assigns to each sample a set of target labels. This can be thought of as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these.

What we will cover (broadly):

* Architecture of a NN classification model
* Input and output shapes of a classification model
* Creating custom data to view and fit
* Steps in the modelling process
* Different classification and evaluation methods
* Saving and loading models

### Classification Inputs & Outputs Part 1

Perhaps we are building an app that will take photos and classify an image as sushi, steak, or pizza. Our input will be images of food and the output will tell us the food that it is. 

When it comes to images, the inputs cannot be photos. We need to numerically encode the images. We could have a 224 x 224 pixel image and a 3 color channels (RGB). The numerical inputs will typically have the same width and height when dealing with images. The images for the input get turned into a tensor, into numerical encoding. In this case, it will be numbers between 0 and 1 for their values of RGB. 

The output will be probabilities for each class, so 3 in this case, with the highest probability assigning the image to the class. 

### Classification Inputs & Outputs Part 2

What will the shape of our inputs and outputs be for an image classification example?

The width and height of our image is 224 x 224 with 3 color channels. The dimensions of our input tensor could be:

* Batch size
* Width
* Height
* Color Channels

The shape of the tensor can be: 

* `Shape` = `[None, 224, 224, 3]`
* `Shape` = `[32, 224, 224, 3]`

What is batch size? Depending on how big our data set is and the amount of computing power we have present, we may only be able to look at 32 samples at one time. For example, if we are working with 10,000 images, our algorithm will only look at 32 at once. 

The shape of the output will be 3 in our case. We are dealing with multiclass classification and we have 3 classes. 

The input and output shapes will vary depending on the problem we are working on. So, if the image is 300 x 300, the width and height will change. If we have 10 possible categories, then the shape of the output will change to 10. 

### Architecture of NN Classification Models

We will have an input layer, one activation function, and then another activation function in the output layer. Furthermore, we will be changing the loss function, metrics, and the input shape. 

The hyperparameters are for binary classification:

* Input layer shape - same as the number of features - same for multiclass
* Hidden layers - problem specific; minimum = 1 and maximum = unlimited - same for multiclass
* Neurons per hidden layer - problem specific; generally 10 to 100 - same for multiclass
* Output layer shape - 1 for binary (one class or the other) - for multiclass, 1 per class (such as 3 for food, person, or dog photo)
* Hidden activation - Usually ReLU - same for multiclass
* Output activation - Sigmoid - for multiclass, it is softmax
* Loss function - cross entropy for both, but the code is different between binary and multiclass
* Optimizer - SGD or Adam for both 

### Creating & Viewing Classification Data to Model

Now that we have covered the basics, let's start to code. In this section we will be creating and viewing classification data to model.
"""

from sklearn.datasets import make_circles

# Make 1000 examples of data
n_samples = 1000

# Create circles
# This will make a large circle containing a small circle in 2d
# It is a simple toy dataset to visualize clustering and classification algorithms
X, y = make_circles(n_samples,
                    noise = 0.03,
                    random_state = 42)

# Check out the features
X

# Check the labels
# This is a binary classification problem 
y[:10]

"""Now, let's visualize some of the data so that we know what we are working with. Currently, our data is a little hard to understand right now."""

import pandas as pd
circles = pd.DataFrame({'X0':X[:, 0], 'X1':X[:, 1], 'label':y})

# View data frame
circles

"""So, we have 2 features per label. We have 1000 rows, as we specified, and 3 columns. Now, let's visualize it."""

# Visualize with a plot
import matplotlib.pyplot as plt
plt.scatter(X[:, 0], X[:, 1], c = y, cmap = plt.cm.RdYlBu)

"""So, the package did make a large circle around a smaller circle. We will try to classify whether the dots are 0 or 1/red or blue. 

What is the difference between the data we are using here versus the data we are using for the regression problem we did before? 

The data here is non-linear and the data we used for the regression problem was linear. Furthermore, here the outcome is either 1 or 0, so discrete values, whereas for the regression problem the outcome was numeric. 

### Checking the Input & Output Shapes of Our Classification 


"""

# Check the shapes of our features and labels
X.shape, y.shape

# How many samples we are working with
len(X), len(y)

# View the first example of features and labels
X[0], y[0]

"""### Steps in Modelling

* Creating the model
* Compiling the model
* Fitting the model
* Evaluating the model
* Tweak 
* Evaluate
* Repeat

### Building A Mediocre Classification Model

Now, we we are going to create, compile, and fit the model.
"""

# Import
import tensorflow as tf

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model using the sequential API
model_1 = tf.keras.Sequential()
model_1.add(tf.keras.layers.Dense(1))

# 2. Compile the model
# Here is where our classification model will differ from our regression model
model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                 optimizer = tf.keras.optimizers.SGD(),
                 metrics = ['accuracy'])

# We will use accuracy, but we can also use ROC, AUC, confusions matrices precision, recall, F1 score
# Accuracy = how many examples did our model get correct

# 3. Fit the model
model_1.fit(X, y, epochs = 5)

"""So, as we can see, the accuracy is low. It seems that our model is just randomly guessing the label. How can we improve our model? We can train it for longer"""

# Increase the number of epochs
model_1.fit(X, y, epochs = 200, verbose = 0)

# Evaluate the model
# Here we are evaluating on the same data we fit the model on
# In real-world scenarios, we fit the model to training data and evaluate it on testing data
model_1.evaluate(X, y)

"""Our model is still not performing well. Since we are working on a binary classification problem and our model is getting around 50% accuracy, it is performing as if it's guessing.

Let's step things up a notch and add an extra layer. 
"""

# Import
import tensorflow as tf

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model using the sequential API
model_2 = tf.keras.Sequential()
model_2.add(tf.keras.layers.Dense(1))
model_2.add(tf.keras.layers.Dense(1))

# 2. Compile the model
# Here is where our classification model will differ from our regression model
model_2.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                 optimizer = tf.keras.optimizers.SGD(),
                 metrics = ['accuracy'])

# We will use accuracy, but we can also use ROC, AUC, confusions matrices precision, recall, F1 score
# Accuracy = how many examples did our model get correct

# 3. Fit the model
model_2.fit(X, y, epochs = 200, verbose = 0)

# 4. Evaluate the model
model_2.evaluate(X, y)

"""Our model is still just as good as guessing. In the next video, we will try and improve our classification model.

### Trying to Improve Our Model

Despite adding an extra layer, model 2 is still performing very poorly.

1. Create a model - add more layers or increase the number of hidden units within a layer.
2. Compiling a model - here we might to choose a different optimization function, such as Adam instead of SGD.
3. Fitting a model - perhaps we might fit our model for more epochs (leave it training for longer).

Other common ways of improving the model:

* Change activation functions
* Change the learning rate
* Fit on more data
"""

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model (this time 3 layers and more hidden units per layer)
model_3 = tf.keras.Sequential()
model_3.add(tf.keras.layers.Dense(100))
model_3.add(tf.keras.layers.Dense(10))
model_3.add(tf.keras.layers.Dense(1))

# 2. Compile the model
model_3.compile(loss = tf.keras.losses.BinaryFocalCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ['accuracy'])

# 3. Fit the model
model_3.fit(X, y, epochs = 100, verbose = 0)

# 4. Evaluate the model
model_3.evaluate(X, y)

"""The accuracy of our model is still bad. Our model is essentially guessing. What else can we visualize? In the next video, we will visualize our predictions. 

### Creating a Function to View Model's Poor Predictions

Let's try to visualize the predictions our model is making.
"""

# Making predictions
model_3.predict(X)[:20]

"""This is not very helpful. So, let's visualize the predictions against the actual data. 

To visualize our model's predictions, let's create a function `plot_decision_boundary()`. 

This function will:

* Take in a trained model, features (X) and (y)
* Create a meshgrid using `numpy` of the different X values
* Make predictions across the meshgrid
* Plot the predictions as well as a line between the different zones, where each unique class falls
"""

# Import numpy
import numpy as np

# Making the function
def plot_decision_boundary(model, X, y):

  # Define the axis boundaries of the plot and create a meshgrid
  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                      np.linspace(y_min, y_max, 100))
  
  # Create x values (we are going to make predictions on these)
  x_in = np.c_[xx.ravel(), yy.ravel()] # Stack 2-d arrays together

  # Make predictions
  y_pred = model.predict(x_in)

  # Check for multiclass
  if len(y_pred[0]) > 1:
    print('doing multiclass classification')
    # We have to reshape our prediction to get them ready for plotting
    y_pred = np.argmax(y_pred, axis = 1).reshape(xx.shape)
  else:
    print('doing binary classification')
    y_pred = np.round(y_pred).reshape(xx.shape)
    
  # Plot the decision boundary
  plt.contourf(xx, yy, y_pred, cmap = plt.cm.RdYlBu, alpha = 0.7)
  plt.scatter(X[:, 0], X[:, 1], c = y, s = 40, cmap = plt.cm.RdYlBu)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())

# Let's run the function by checking the predictions our model is making
plot_decision_boundary(model_3,
                       X = X,
                       y = y)

"""Sp, what does this plot tell us? It is showing us that our model is performing so poorly because it is trying to draw a straight line through the data. Our data is circuluar and it is not separable by a straight line. In a regression problem, our model might work since it is predicting a straight line. So, how do we improve this? 

### Make Our Model Work For a Regression Dataset

All of our previous models are performing poorly, as good as guessing. Our data is circular, but our model is predicting a straight line. 
"""

# Let's see if our model can be used for a regression problem

tf.random.set_seed(42)

# Create some regression data
X_regression = tf.range(0, 1000, 5)
y_regression = tf.range(100, 1100, 5)# y = x + 100

# Inspect
X_regression, y_regression

# Split our regression data into training and test
X_reg_train = X_regression[:150]
X_reg_test = X_regression[150:]
y_reg_train = y_regression[:150]
y_reg_test = y_regression[150:]

# We cannot fit our model to the regression data as it was previously
# We will have a shape issue. Our model is made for a binary classification problem not regression
# Our loss function is binary cross entropy and this needs to change 

# model_3.fit(X_reg_train, y_reg_train, epochs = 100)

"""Let's recreate model 3 but change the loss function to an appropriate function for a regression problem."""

# Setup random seed
tf.random.set_seed(42)

# Recreate the model 
model_3 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Change the loss and metrics of our compiled model
model_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae']) # change the metric to be regression-specific

# Fit the recompiled model
model_3.fit(tf.expand_dims(X_reg_train, axis=-1), 
            y_reg_train, 
            epochs=100)

"""Seems like our model is learning something from these training metrics. 

Now, let's make predictions with our trained model.
"""

from pandas.compat import platform
# Make predictions
y_reg_preds = model_3.predict(X_reg_test)

# Plot the model's predictions against our regression data
plt.figure(figsize = (10, 7))
plt.scatter(X_reg_train, y_reg_train, c = 'b', label = 'Training Data')
plt.scatter(X_reg_test, y_reg_test, c = 'g', label = 'Test Data')
plt.scatter(X_reg_test, y_reg_preds, c = 'r', label = 'Predictions')
plt.legend();

"""So, our predictions are not too far off. This means that our model is learning something, but it is still missing something for our classification problem. Our regression problem is a straight line and our classification data is non-linear, but the decision boundary our model is plotting is linear. We have to introduce non-linearity in our models. 

### Non-linearity Part 1

We have seen that our built NN can model straight lines with better than guessing accuracy. However, when it comes to non-linear data, we are missing something. The thing we are missing is non-linearity. Neural networks can model non-linear data using activation functions in the hidden layers and output layer. For the hidden layers, we can use a selection of activation functions, with ReLU being the most popular. For the output layer, depending on the number of outcomes, we can use either sigmoid or softmax.

Let's look at changing activation functions.


"""

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model
model_4 = tf.keras.Sequential()
model_4.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.linear))


# 2. Compile our model
model_4.compile(loss = 'binary_crossentropy', # We can use strings to set arguments
                optimizer = tf.keras.optimizers.Adam(lr = 0.001),
                metrics = ['accuracy']
                )

# 3. Fit the model 
history = model_4.fit(X, y, epochs = 100)

"""Looks like our model is performing a little worse than guessing. Let's remind ourselves what our data looks like."""

# Check out our data
plt.scatter(X[:, 0], X[:, 1], c = y, cmap = plt.cm.RdYlBu);

# Check the decision boundary for our latest model
plot_decision_boundary(model = model_4, 
                       X = X,
                       y = y)

"""Our model's decision boundary is all over the place. The yellow shade is showing that anything there could be blue or red. Our model is still predicting straight lines. Let's use activation functions to fix this.

### Non-linearity Part 2:

Let's add the ReLU activation function, a non-linear activation function, in our neural network.
"""

# Set random seet
tf.random.set_seed(42)

# 1. Create a model with a non-linear activation
model_5 = tf.keras.Sequential()
model_5.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.relu))

# 2. Compile the model
model_5.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(lr = 0.001),
                metrics = ['accuracy'])

# 3. Fit the model
history = model_5.fit(X, y, epochs = 100)

"""Even with our non-linear activation function, our model is still performing as good as guessing. Now, let's add more neurons and layers to our NN. 

### Non-linearity Part 3:

In this tutorial, we will add more neurons and layers to our NN, while also changing the activation functions.
"""

# Set random seed
tf.random.set_seed(42)

# 1. Create the model
model_6 = tf.keras.Sequential()
model_6.add(tf.keras.layers.Dense(4, activation = tf.keras.activations.relu))
model_6.add(tf.keras.layers.Dense(4, activation = 'relu')) # Can also use qutations marks to set the activation like this

# 2. Compile the model
model_6.compile(loss = 'binary_crossentropy',
                optimizer = tf.keras.optimizers.Adam(lr = 0.001),
                metrics = ['accuracy'])

# 3. Fit the model
history = model_6.fit(X, y, epochs = 100)

"""Looks like our model is now performing worse than guessing. Let's evaluate the model."""

# Evaluating the model
model_6.evaluate(X, y)

"""Maybe we need to train our model for longer?"""

# Set random seed
tf.random.set_seed(42)

# 1. Create the model
model_7 = tf.keras.Sequential()
model_7.add(tf.keras.layers.Dense(4, activation = tf.keras.activations.relu))
model_7.add(tf.keras.layers.Dense(4, activation = 'relu')) # Can also use qutations marks to set the activation like this

# 2. Compile the model
model_7.compile(loss = 'binary_crossentropy',
                optimizer = tf.keras.optimizers.Adam(lr = 0.001),
                metrics = ['accuracy'])

# 3. Fit the model
history = model_7.fit(X, y, epochs = 250, verbose = 0)

# Evaluate
model_7.evaluate(X, y)

"""Our model is still basically guessing. Let's see what happens if we fix it so the last layer has 1 option rather than 4, since this is a binary classification problem."""

# Set random seed
tf.random.set_seed(42)

# 1. Create the model
model_8 = tf.keras.Sequential()
model_8.add(tf.keras.layers.Dense(4, activation = tf.keras.activations.relu))
model_8.add(tf.keras.layers.Dense(4, activation = 'relu')) # Can also use qutations marks to set the activation like this
model_8.add(tf.keras.layers.Dense(1))

# 2. Compile the model
model_8.compile(loss = 'binary_crossentropy',
                optimizer = tf.keras.optimizers.Adam(lr = 0.001),
                metrics = ['accuracy'])

# 3. Fit the model
history = model_8.fit(X, y, epochs = 250, verbose = 0)

# Evaluate
model_8.evaluate(X, y)

"""Our accuracy is still 50% even after 250 epochs. The model is still essentially just guessing. Let's visualize what is happening. """

# How do our model predictions look?
plot_decision_boundary(model_8, X, y)

"""It looks like our model is still operating from straight lines. It seems that we need to change the activation function for the last layer. The last layer, which is the output layer, needs to be changed so that the output is non-linear. In a binary classification problem, we will be using a sigmoid activation function. 

### Non-linearity Part 4: 

Let's add the sigmoid activation function to the output layer and see how our model performs. 
"""

# Set the random seet
tf.random.set_seed(42)

# 1. Create the model
model_9 = tf.keras.Sequential() 
model_9.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_9.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_9.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.sigmoid))

# 2. Compile the model
model_9.compile(loss = 'binary_crossentropy',
                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
                metrics = ['accuracy'])

# 3. Fit the model
history = model_9.fit(X, y, epochs = 100)

"""As you can see, our accuray has gone up to about 99%. Let's try evaluating the model."""

# Evaluate the model
model_9.evaluate(X, y)

# Let's visualize our metrics
plot_decision_boundary(model_9, X, y)

"""As you can see, our model is performing really well. It seems that our model has found a really good decision boundary between our red and blue points. 

As a note, we do not having training and testing sets, so our model is not being evaluated in the correct manner. This is not super relevant right now because we are just practicing. 

The combination of linear and non-linear functions is one of the most important aspects of neural networks. The activation functions in the hidden layers and output layers allow the network to learn non-linear patterns. 

### Non-linearity Part 6:

In this tutorial, we will replicate non-linear activations functions from scratch. We have seen the names of the functions, but we have not seen the details of the activation functions. 
"""

# Create a toy tensor similar to our data we pass into our models
A = tf.cast(tf.range(-10, 10), tf.float32)
A

# Visualize our toy tensor
plt.plot(A);

"""Let's apply the sigmoid activation function to our tensor. The sigmoid activation function is:

$$ sigmoid(x) = 1 / (1 + e^{-x}) $$
"""

# Let's make the sigmoid function
def sigmoid(x):
  return 1 / (1 + tf.exp(-x))

# Use the sigmoid function on the toy tensor
sigmoid(A)

# Plotting the sigmoid of A
plt.plot(sigmoid(A));

"""This line was originally straight, but it is not in an S shape. The sigmoid function takes any real number and plots it to probability between 0 and 1. The sigmoid function is called a squashing function because the output is always between 0 and 1. 

Now, let's look at the ReLU activation function. ReLU is the rectified linear unit. The ReLU activation function will output the input directly if it is positive, otherwise it will output 0. 
"""

# Let's create the relu function
def relu(x):
  return tf.maximum(0, x)

# Let's pass our toy tensor to our custom relu function
relu(A)

# Let's plot our toy tensor with the ReLU function
plt.plot(relu(A));

"""The ReLU is used in the hidden layers instead of sigmoid or tanh as using sigmoid or tanh in the hidden layers leads to the infamous problems of the vanishing gradient. The vanishing gradient prevents the earlier layers from learning important information when the network is backpropagating. 

Let's try the linear activation function. In tensorflow, the linear activation will return the tensor unmodified.
"""

# Linear activation function
tf.keras.activations.linear(A)

# Does the linear activation function change anything?
plt.plot(tf.keras.activations.linear(A));

# Does A even change?
A == tf.keras.activations.linear(A)

"""Our linear activation function essentially changes nothing, that is why our results previously were so bad. Using non-linear functions allows the neural network to learn patterns present in non-linear data. 

### Tweaking the Learning Rate

In the last few videos, we tackled the concept of non-linearity. The combination of straight line functions and non-straight line functions is a key concept in neural networks. In this tutorial, we will evaluate our model and improve our classification. Furthermore, we will use a training set and a testing set, which we did not use previously. 

The goal of our NN is to generalize, which means we want to it perform well on data it has never seen before.

Let's create a training and test set.

"""

# Check how many examples we have
len(X)

# Let's create train/test sets by indexing
X_train, y_train = X[:800], y[:800]
X_test, y_test = X[800:], y[800:]

X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Let's create a model to fit on the train data and evaluate on the testing data

# Set the seed
tf.random.set_seed(42)

# 1. Create the model
model_10 = tf.keras.Sequential()
model_10.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_10.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_10.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))

# 2. Compile the model
model_10.compile(loss = 'binary_crossentropy', # The loss function tells the model how wrong the predictions are
                 optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01), # The optimizer tells the model how it should improve - the learning rate is how much the model should improve those patterns
                 # The learning rate determines the step size at each iteration while moving toward a minimum of a loss function - the larger the learning rate, the more our model updates the parameters
                 metrics = ['accuracy'])

# 3. Fit the model
history = model_10.fit(X_train, y_train, epochs = 25)

"""As you can see, with our new learning rate, we are at an accuracy of 98% after only 25 epochs. Now, let's evaluate the model on the test data."""

model_10.evaluate(X_test, y_test)

# Plotting the decision boundaries for the training and test sets
plt.figure(figsize = (12,6))
plt.subplot(1, 2, 1)
plt.title('Train')
plot_decision_boundary(model_10, X = X_train, y = y_train)
plt.subplot(1, 2, 2)
plt.title('Test')
plot_decision_boundary(model_10, X = X_test, y = y_test)
plt.show;

"""### Using the TensorFlow History Object to Plot a Model's Loss Curves

We have been our saving our model into a `history` object. Let's see how we can plot the loss (or training) curves using this object. 
"""

# history.history will just give us the loss and accuracy; it essentially just tracks what is happening when the model is being trained
history.history

# Let's make our history object into a data frame
pd.DataFrame(history.history)

# Plot the loss curves
pd.DataFrame(history.history).plot()
plt.title('Model_8 Loss Curves')

"""This is an ideal version of what we want to see in a classification problem. As you can see, the loss (how wrong our model is) is decreasing, which means our model is improving. Furthermore, our accuracy is going up (the predictions it's making are getting closer to the ground truth labels).

### Using Callbacks to Find a Model's Ideal Learning Rate

We have seen how the learning rate can affect our model's training. Is there a method to find an ideal learning rate? A value which would allow the model's loss during training to decrease as fast as possible. 

How much we decrease the learning rate during training? To find the ideal learning rate (the learning rate where the loss decreases the most during training), we are going to use the following steps:

1. A learning rate callback - you can think of a callback as an extra piece of functionality you can add to your model while its training. 
2. Another model (we could use the same as above, but we are practicing building models here).
3. A modified loss curves plot. 
"""

# Set random seed
tf.random.set_seed(42)

# 1. Create a model
model_11 = tf.keras.Sequential()
model_11.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_11.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_11.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))

# 2. Compile the model
model_11.compile(loss = 'binary_crossentropy',
                 optimizer = 'Adam',
                 metrics = ['accuracy'])

# A callback works just during model training
# In order for the callback to run, it must exist before fitting the model
# Create a learning rate call back
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) 
# For every epoch, it will traverse a set of learning rate values starting from 
# 1e-4 and increasing by 10**(epoch/20) every epoch

# 3. Fit the model (passing lr_scheduler callback)
history_11 = model_11.fit(X_train, 
                          y_train,
                          epochs = 100,
                          callbacks = [lr_scheduler]) # Callbacks are in list form since there can be many

# Checkout the history
pd.DataFrame(history_11.history).plot(figsize = (10, 7), xlabel = 'epochs');

"""The y-axis above is the learning rate. As we increase in epochs, the learning rate increases. Our accuracy seems to up and then decreases as we go into more epochs. Our loss decreases then increases again as we go further. 

We want a learning rate where the loss decreases the fastest. This ideal learning rate is achieved at around 40-50 epochs. Let's plot the learning rate values during training versus the loss.
"""

# Plot the learning rate versus the loss
# We are replicating what we are passing to the learning rate scheduler so we can plot it
# These are the learning rates our model tried out
lrs = 1e-4 * (10 ** (tf.range(100)/20))
len(lrs)

# Plotting
plt.figure(figsize = (10,7))
plt.semilogx(lrs, history_11.history['loss'])
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.title('Learning Rate Versus Loss')

"""We want a learning rate value where our loss decreases the most. To figure out the ideal value of the learning rate, the rule of thumb is to take the learning rate value where the loss is still decreasing but not flattened out. In our case, the ideal learning rate would be about 10^-2, which is the default learning rate we use. 

The pre-built optimizers have generally very good default parameters. When they don't, you can use other learning rates. You typically do not use 1, but rather the options below 1. Additionally, you can also plot the learning rate versus loss, as we did above. 
"""

# Example of other typical learning rate values are:
10**0, 10**-1, 10**-2, 10**-3, 1e-4

"""### Training & Evaluating a Model With an Ideal Learning Rate

Let's train a model with a learning rate of of 0.02. We got this value by using the plot we made in the last tutorial.
"""

# Set seed
tf.random.set_seed(42)

# 1. Create the model
model_12 = tf.keras.Sequential()
model_12.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_12.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_12.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))

# 2. Compile the model with the ideal learning rate (0.02)
model_12.compile(loss = 'binary_crossentropy',
                 optimizer = tf.keras.optimizers.Adam(learning_rate = 0.02),
                 metrics = ['accuracy'])

# 3. Fit the model for 20 epochs (5 less than before)
history_12 = model_12.fit(X_train, y_train, epochs = 20)

"""Our model with the ideal learning rate achieved an accuracy of 99% after only 20 epochs. Compared to model 10, which had 25 epochs and a learning rate of 0.01, our current model, model 12, achieved a greater accuracy in less time."""

# Evaluate model 12 on the test dataset
model_12.evaluate(X_test, y_test)

# Evaluate model 10 on the test data
model_10.evaluate(X_test, y_test)

"""When evaluating the 2 models on the testing data, model 12 gets a lower loss and an accuracy of 99%. However, model 10 gets a higher accuracy (100%), but has a higher loss. This is something that should be investigated more, depending on what your needs are. It can take some trial and error to see what is needed for your use-case."""

# Plot the decision boundaries for the training and test sets
plt.figure(figsize = (12, 6))
plt.subplot(1, 2, 1)
plt.title('Train')
plot_decision_boundary(model_12, X = X_train, y = y_train)
plt.subplot(1, 2, 2) # Row, column, section
plt.title('Test')
plot_decision_boundary(model_12, X = X_test, y = y_test)
plt.show()

"""### Introducing More Classification Evaluation Methods

Some common evaluation metrics are:

* Accuracy - Default metric for classification problems. Not the best for imbalanced classes.
* Precision - Higher precision leads to less false positives.
* Recall - Higher recall leads to less false negatives.
* Precision-Recall Tradeoff - You cannot have both precision and recall high. If you increase precision, it will reduce recall, and vice versa. 
* F1-Score - Combination of precision and recall, usually a good overall metric for a classification model. 
* Confusion Matrix - When comparing predictions to truth labels to see where the model gets confused. Can be hard to use with a large number of classes. 
* Classification Report (from `sci-kit` learn). 

### Finding the Accuracy of Our Classification Model

Our model has already used accuracy as it's metrics. However, let's make it nicer.
"""

# Check the accuracy of our model
loss, accuracy = model_12.evaluate(X_test, y_test)
print(f'Model loss on the test set: {loss}')
print(f'Model accuracy on the test set: {(accuracy*100):.2f}%')

"""### Creating Our First Confusion Matrix

The anatomy of a confusion matrix is:




![image.png](data:image/png;base64,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)

The diagonal contains the correct predictions: true positives and the true negatives. 

* True positive = model predicts 1 when the truth is 1
* True negative = model predicts 0 when the truth is 0
* False positive = model predicts 1 when the truth is 0
* False negative = model predicts 0 when the truth is 1
"""

# Now let's make a confusion matrix using sci-kit learn
from sklearn.metrics import confusion_matrix

# Make some predictions
# As a note, you may get a value error when making the confusion matrix if you do not 
# Fix the values of the y predictions
# The test values are in binary form, but the predictions will be in numeric form 
# The prediction array will need to be converted to 0s and 1s 
y_preds = model_12.predict(X_test)

"""The prediction array is in prediction probability form, the standard output from the sigmoid or softmax activation functions. Thus, we need to convert these values to binary outcomes. The cut-off we typically use 0.5, but this may change depending on the use-case."""

# Convert our prediction probabilities to binary format and view the first 10
tf.round(y_preds)[:10]

# Create the confusion matrix
confusion_matrix(y_test, tf.round(y_preds))

"""### Making Our Confusion Matrix Prettier


"""

# Note: The confusion matrix code we are going to write is a remix of the scikit-learn 
# plot_confusion_matrix function
import itertools

figsize = (10, 10)

# Create the confusion matrix 
cm = confusion_matrix(y_test, tf.round(y_preds))

# Create a normalized feature to give us percentages
cm_norm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]

# Set the number of classes
n_classes = cm.shape[0]

# Let's make the confusion matrix prettier
fig, ax = plt.subplots(figsize = figsize)

# Create a matrix plot and use a color map of blues
cax = ax.matshow(cm, cmap = plt.cm.Blues)
fig.colorbar(cax)

# Create classes
classes = False # Setting up a boolean in case it is multiclass

if classes:
  labels = classes 
else: 
  labels = np.arange(cm.shape[0])

# Label the axes
ax.set(title = 'Confusion Matrix',
       xlabel = 'Predicted Label',
       ylabel = 'True Label',
       xticks = np.arange(n_classes),
       yticks = np.arange(n_classes),
       xticklabels = labels,
       yticklabels = labels)

# Set x-axis labels to the bottom
ax.xaxis.set_label_position('bottom')
ax.xaxis.tick_bottom()

# Adjust label size
ax.yaxis.label.set_size(20)
ax.xaxis.label.set_size(20)
ax.title.set_size(20)

# Set the threshold for different colors
threshold = (cm.max() + cm.min()) / 2.

# Plot the text in each cell
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
  plt.text(j, i, f'{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)',
           horizontalalignment = 'center',
           color = 'white' if cm[i, j] > threshold else 'black',
           size = 15)

"""# Multi-Class Classification

### Multi-Class Classification Part 1: Getting the Data

Now that we have completed an example using a binary classification problem, we will now work through a multi-class classification problem. 

When you have more than 2 classes as an option, it is known as a multi-class classification problem. 

* This means if you have 3 different classes, it is multi-class classification.
* It also means if you have 100 different classes, it is multi-class classification. 

The good news is that we can apply many of the same concepts we learned during binary classification, except with a few tweaks. 

We will pretend we are a fashion company and we want to build a NN to classify different images of clothing. We can use the fashion MNIST dataset, which is built into `TensorFlow`. 

This dataset has:

* 60,000 training examples
* 10,000 test examples
* Each example is a 28x28 grayscale image, associated with a label from 10 classes. 
"""

import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist

# The data has already been sorted into training and test sets for us
# To import it, we can use tuples
(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()

# Show the first training example
print(f'Training sample:\n{train_data[0]}\n')
print(f'Training label:\n{train_labels[0]}\n')

"""There are 10 classes, ranging from 0 to 9. In this case, the above example is an ankle boot. """

# Check the shape of a single example
train_data[0].shape, train_labels[0].shape

"""So, each example is 28x28 and the train label has no shape since it is just a scalar."""

# Plot a single sample
import matplotlib.pyplot as plt
plt.imshow(train_data[0]);

# Plot a single sample again
import matplotlib.pyplot as plt
plt.imshow(train_data[3]);

# Check out the above samples label - it will be a dress
train_labels[3]

"""### Multi-Class Classification Part 2: Exploring the Data

Let's get familiar with the data that we will be working with for the multi-class classification problem. 

Our labels are numeric, which is fine for NN, but we want them in human readable form so we can index on the list and find the item associated with the number.
"""

# Create a small list so we can index onto our training labels so they are human readable
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 
               'Bag', 'Ankle Boot']

# How many classes are we dealing with? 
len(class_names)

# Plot an example imahe and its label
index_of_choice = 17
plt.imshow(train_data[index_of_choice], cmap = plt.cm.binary) 
# Colormap will be binary because it is grayscale
plt.title(class_names[train_labels[index_of_choice]])

# Let's setup some code to plot random examples of fashion MNIST
import random
plt.figure(figsize = (7, 7))

for i in range(4):
  ax = plt.subplot(2, 2, i+1)
  rand_index = random.choice(range(len(train_data)))
  plt.imshow(train_data[rand_index], cmap = plt.cm.binary)
  plt.title(class_names[train_labels[rand_index]])
  plt.axis(False)

"""With this type of data, will we need a neural network that works only with straight lines? Or will we need a neural network that is going to have some non-linearity in it? 

### Multi-Class Classification Part 3: Building a Multi-Class Classification Model

For our multi-class classification model, we need to change:

* Input shape - this is 28 x 28, which is the shape of one image.
* Output layer shape - this needs 1 per class.
* Output activation - this will be the softmax activation function rather than sigmoid.
* Loss function - this will be the categorical cross entropy rather than binary cross entropy. In this example, this will be 10, since we have 10 classes of clothing.

Otherwise, the architecture will be similar to our binary classification model. 
"""

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model
model_13 = tf.keras.Sequential()
# This is telling the NN we are passing images that are 28 x 28
# Our data needs to be flattened from (28 x 28) to (None, 784)
model_13.add(tf.keras.layers.Flatten(input_shape = (28, 28)))
model_13.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_13.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_13.add(tf.keras.layers.Dense(10, activation = 'softmax')) # Can also do tf.keras.activations.softmax


# 2. Compile the model 
model_13.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer = tf.keras.optimizers.Adam(),
                 metrics = ['accuracy'])

# 3. Fit the model
non_norm_history = model_13.fit(train_data, 
                                train_labels,
                                epochs = 10,
                                validation_data = (test_data, test_labels))
# The validation_data = argument allows us to evaluate the training predictions on 
# the validation/test data

"""So, what does the flatten layer do in the NN? According to the documentation, this layer flattens the input (and it does not affect batch size). Our 28 x 28 images will be flattened to 28*28 = 784. Essentially, this layer converts the 2-dimensional object into a 1-dimensional vector because this is the ideal format for NNs.

If you ever run into a shape error, check if using a flatten layer for this first layer will work. 

Furthermore, we need to be careful with our loss functions. There are 2 types of loss functions.

* One is if your data is one-hot encoded. `tf.keras.losses.CategoricalCrossentropy()` expects labels to be provided in a `one_hot` representation. 
* The second is for integers. `tf.keras.losses.SparseCategoricalCrossentropy()` is to be used when you want ro provide labels as integers. 
"""

# If we wanted to one-hot encode our labels and use the categorical cross entropy, we could
tf.one_hot(train_labels, depth = 10) # depth = is the number of classes

"""In addition to this, if we are one-hot encoding, we would also need to one hot encode the labels in the test data. It would look like:

* `validation_data = (test_data, tf.one_hot(test_labels, depth = 10))`

### Multi-Class Classification Part 4: Improving Performance With Normalization

We coded the `validation_data` argument, but we did not really explain it. The validation loss and accuracy in the output is telling us how our model is performing on data it has never seen before. Whenever you pass the argument, you will get extra output when training the model. 

Right now, our model the accuracy of our model is about 35%, which is better than guessing. Can we improve this through normalization?
"""

# Check the model summary
model_13.summary()

"""In previous videos, we talked about scaling and normalization. Remember, NNs tend to prefer normalization. This means they like to have numbers in the tensors they to find patterns in between 0 and 1. Right now our data is between:"""

# Check the min and max values of the training data
train_data.min(), train_data.max()

"""How do we get our data between 0 and 1? """

# We can our training and testing data between 0 and 1 by dividing by the maximum
train_data_norm = train_data / 255.0
test_data_norm = test_data / 255.0

# Check the min and max values of the scaled training data
train_data_norm.min(), train_data_norm.max()

# Let's make our model with the normalized data

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model
model_13 = tf.keras.Sequential()
model_13.add(tf.keras.layers.Flatten(input_shape = (28, 28)))
model_13.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_13.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_13.add(tf.keras.layers.Dense(10, activation = 'softmax')) # Can also do tf.keras.activations.softmax


# 2. Compile the model 
model_13.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer = tf.keras.optimizers.Adam(),
                 metrics = ['accuracy'])

# 3. Fit the model
norm_history = model_13.fit(train_data_norm, 
                                train_labels,
                                epochs = 5,
                                validation_data = (test_data_norm, test_labels))

"""As we can see from the model above, our model has an accuracy of 79% on the test data ater only 5 epochs. Our previous model with non-normalized data used 10 epochs and the accuracy was 33% on the test data.

### Multi-Class Classification Part 5: Comparing Normalized & Non-Normalized Data 

Let's compare the loss curves of each model.
"""

import pandas as pd

# Plot non-normalized data loss curves
pd.DataFrame(non_norm_history.history).plot(title = 'Non-normalized data')

# Plot normalized data loss curves
pd.DataFrame(norm_history.history).plot(title = 'Normalized data')

"""From these 2 plots, we can see how much quicker the model with normalized data improved when compared to the model with normalized data. 

When making comparisons of different models, the same model with even slightly different data can produce dramatically different results. So when you are comparing models, it is important to make sure you are comparing them on the same criteria (e.g., same architecture but different data or same data but different architecture. 

### Multi-Class Classification Part 6: Finding the Ideal Learning Rate

The ideal learning rate is the learning rate where the loss decreases the most.
"""

import tensorflow as tf

# Set the seed
tf.random.set_seed(42)

# 1. Create the model
model_14 = tf.keras.Sequential()
model_14.add(tf.keras.layers.Flatten(input_shape = (28, 28)))
model_14.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_14.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_14.add(tf.keras.layers.Dense(10, activation = 'softmax')) # Can also do tf.keras.activations.softmax


# 2. Compile the model 
model_14.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer = tf.keras.optimizers.Adam(),
                 metrics = ['accuracy'])

# Create the learning rate callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))
# We are starting at 1e-3 and increasing the learning rate at every epoch by 10 ** (epoch / 20)

# 3. Fit the model
find_lr_history = model_14.fit(train_data_norm, 
                                train_labels,
                                epochs = 10,
                                validation_data = (test_data_norm, test_labels),
                                callbacks = [lr_scheduler])

# Plot the learning rate decay curve
import numpy as np
import matplotlib.pyplot as plt

lrs = 1e-3 * (10 ** (tf.range(10) / 20))
plt.figure(figsize = (10, 7))
plt.semilogx(lrs, find_lr_history.history['loss'])
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.title('Finding the Ideal Learning Rate')

"""We can see that, when it started off, the loss decreased fairly sharply and continued to do so. In the tutorial on Udemy, 40 epochs were run and, after awhile, the loss would increase as the learning rate increased. 

The ideal learning rate is where the loss decreasing sharply. Find the lowest point in the curve and go back slightly. Our ideal learning right is 10e-3, which is 0.001, the default parameter for the Adam optimizer. 

Let's refit a model with the ideal learning rate. 
"""

# Set the seed
tf.random.set_seed(42)

# 1. Create the model
model_15 = tf.keras.Sequential()
model_15.add(tf.keras.layers.Flatten(input_shape = (28, 28)))
model_15.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_15.add(tf.keras.layers.Dense(4, activation = 'relu'))
model_15.add(tf.keras.layers.Dense(10, activation = 'softmax')) # Can also do tf.keras.activations.softmax


# 2. Compile the model 
model_15.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
                 metrics = ['accuracy'])

# 3. Fit the model
history_15 = model_14.fit(train_data_norm, 
                                train_labels,
                                epochs = 10,
                                validation_data = (test_data_norm, test_labels))

"""### Multi-Class Classification Part 7: Evaluating Our Model

Now that we have a model trained with close to an ideal learning rate and that is performing pretty well, let's evaluate our model. 

To evaluate or test our multi-class classification model, we could: 

* Evaluate its performance using other classification metrics, such as confusion matrix.
* Assess some of its predictions through visualizations
* Improve its results by training it for longer or changing the architecture 
* Save & export it for use in an application 

We will do the first 2, making a confusion matrix and assessing predictions.
"""

# Create a confusion matrix

# Note: The confusion matrix code we are going to write is a remix of the scikit-learn 
# plot_confusion_matrix function
import itertools
from sklearn.metrics import confusion_matrix

# Let's make a function
def make_confusion_matrix(y_true, y_pred, classes = None, figsize = (10, 10), text_size = 15):

  # Create the confusion matrix 
  cm = confusion_matrix(y_true, y_pred)

  # Create a normalized feature to give us percentages
  cm_norm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]

  # Set the number of classes
  n_classes = cm.shape[0]

  # Let's make the confusion matrix prettier
  fig, ax = plt.subplots(figsize = figsize)

  # Create a matrix plot and use a color map of blues
  cax = ax.matshow(cm, cmap = plt.cm.Blues)
  fig.colorbar(cax)

  # Set labels to be classes
  if classes:
    labels = classes 
  else: 
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title = 'Confusion Matrix',
        xlabel = 'Predicted Label',
        ylabel = 'True Label',
        xticks = np.arange(n_classes),
        yticks = np.arange(n_classes),
        xticklabels = labels,
        yticklabels = labels)

  # Set x-axis labels to the bottom
  ax.xaxis.set_label_position('bottom')
  ax.xaxis.tick_bottom()

  # Adjust label size
  ax.yaxis.label.set_size(text_size)
  ax.xaxis.label.set_size(text_size)
  ax.title.set_size(text_size)

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text in each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, f'{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)',
            horizontalalignment = 'center',
            color = 'white' if cm[i, j] > threshold else 'black',
            size = text_size)

# Remind ourselves of class names
class_names

# Let's make predictions with our model; making prediction probabilities
y_probs = model_14.predict(test_data_norm) 

# View the first 5 prediction probabilities
y_probs[:5]

"""As a note, remember to make predictions on the same kind of data your model was trained on. So, if your model is trained on normalized data, it should make predictions on normalized data. """

# Convert all of the prediction probabilities to integers
# This will make our predictions in the same format as our test labels 
y_preds = y_probs.argmax(axis = 1)

# View the first 10 prediction labels
y_preds[:10]

"""### Multi-Class Classification Part 8: Creating a Confusion Matrix


"""

# Make a for loop to count how many prediction labels match true labels
count = 0
for i, j in zip(y_preds, test_labels):
  if i == j:
    count += 1
  else:
    continue

# Get the accuracy
accuracy = count / len(y_preds)
print(accuracy)

"""Now that we have gotten the same accuracy as our model, let's make a confusion matrix using `sci-kit learn`. """

from sklearn.metrics import confusion_matrix
confusion_matrix(y_true = test_labels,
                 y_pred = y_preds)

"""This is a bit much and contains a lot of values. One thing to look is the diagonal. On the diagonal, we want to have the highest numbers because these are our correct predictions."""

# Make a prettier confusion matrix using our function
make_confusion_matrix(y_true = test_labels,
                      y_pred = y_preds,
                      classes = class_names,
                      figsize = (15, 15),
                      text_size = 10)

"""When looking at a confusion matrix like the one above, we can start to see patterns where our model is making mistakes, such as thinking pullovers are coats. This will then give us guidance on how we can possibly improve our model. 

### Multi-Class Classification Part 9: Visualizing Random Model Predictions

When working with images and other forms of visual data, it is a good idea to visualize as much as possible to develop a further understanding of the data and the inputs and outputs of your models. 

How about we create a function for:
* Plot a random image
* Make a prediction on said image
* Label the plot with the true label and the predicted label
"""

import random

def plot_random_image(model, images, true_labels, classes):
  # Set up random integer
  i = random.randint(0, len(images))

  # Create predictions and targets
  target_image = images[i]

  # Passing only one image at a time
  pred_probs = model.predict(target_image.reshape(1, 28, 28))

  pred_label = classes[pred_probs.argmax()]
  true_label = classes[true_labels[i]]

  # Plot the image
  plt.imshow(target_image, cmap = plt.cm.binary)

  # Change the color of the titles depending on on if the prediction is right/wrong
  if pred_label == true_label:
    color = 'green'
  else:
    color = 'red'

  # Add xlabel information (prediction/true label)
  plt.xlabel('Pred: {} {:2.0f}% (True: {})'.format(pred_label, 
                                                   100* tf.reduce_max(pred_probs),
                                                   true_label), 
                                                   color = color)

# Running our function
plot_random_image(model = model_15, 
                  images = test_data_norm, # Always make predictions on the same kind of data your model was 
                  # trained on - preprocessed the same way
                  true_labels = test_labels,
                  classes = class_names)

"""### What Patterns Is Our Model Learning?


"""

# Find the layers of our most recent model
model_15.layers

# Extract a particular layer
model_15.layers[1]

# Get the patterns of a layer in our network
# These are the patterns in the first layer that contribute to the decisions our model is making
weights, biases = model_15.layers[1].get_weights()

# Shapes
weights, weights.shape

# Remind ourselves of our model
model_15.summary()

"""So, how does our neural network learn these patterns? 

* First, our model initializes itself with random weights. 
* Then we are going to show it examples of images that we want to learn.
* It is going to update the representation outputs (weights and biases) through gradient descent and backpropagation. 
* Gradient Descent is an optimization algorithm that finds the set of input variables for a target function that results in a minimum value of the target function, called the minimum of the function.
* Backpropagation is an algorithm for calculating the gradient of a loss function with respect to variables of a model.
"""

# Look at the bias vector 
# Weights have 1 number per data point
# On the other hand, biases have 1 data point per hidden unit
biases, biases.shape

"""Every neuron has a bias vector. Each of these is paired with a weights matrix. 

The bias vector also gets initialized as zeros (at least in the case of a `TensorFlow` dense layer). 

Sometimes, depending on what layer you are using, your weights matrix and bias may be initialized different. We never actually set these variables and they are set by default by `TensorFlow`. 

The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer.

* Inputs: Inputs are the set of values for which we need to predict a output value. They can be viewed as features or attributes in a dataset.
* Weights: weights are the real values that are attached with each input/feature and they convey the importance of that corresponding feature in predicting the final output. (will discuss about this in-detail in this article)
* Bias: Bias is used for shifting the activation function towards left or right, you can compare this to y-intercept in the line equation. (will discuss more about this in this article)
* Summation Function: The work of the summation function is to bind the weights and inputs together and calculate their sum.
* Activation Function: It is used to introduce non-linearity in the model.

For each layer in a neural network, the output from the previous layer is used as the input for the current layer. This is the core idea behind deep learning.
"""

# Let's check out another way of viewing our deep learning models
from tensorflow.keras.utils import plot_model

# See the inputs and outputs of each layer
plot_model(model_14, show_shapes = True)